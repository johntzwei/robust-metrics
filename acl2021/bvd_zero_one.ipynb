{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lp</th>\n",
       "      <th>HITId</th>\n",
       "      <th>WorkerId</th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>system</th>\n",
       "      <th>type</th>\n",
       "      <th>sid</th>\n",
       "      <th>reference</th>\n",
       "      <th>source</th>\n",
       "      <th>...</th>\n",
       "      <th>chrf:statistics_10</th>\n",
       "      <th>chrf:statistics_11</th>\n",
       "      <th>chrf:statistics_12</th>\n",
       "      <th>chrf:statistics_13</th>\n",
       "      <th>chrf:statistics_14</th>\n",
       "      <th>chrf:statistics_15</th>\n",
       "      <th>chrf:statistics_16</th>\n",
       "      <th>chrf:statistics_17</th>\n",
       "      <th>bert_score:f1</th>\n",
       "      <th>bleurt_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>de-en</td>\n",
       "      <td>3QQUBC64ZEEFIPT2SKINDZZTZ3SNXN</td>\n",
       "      <td>A0077</td>\n",
       "      <td>78</td>\n",
       "      <td>2052.0</td>\n",
       "      <td>online-G</td>\n",
       "      <td>SYSTEM</td>\n",
       "      <td>1906</td>\n",
       "      <td>Rather than having an executive make the annou...</td>\n",
       "      <td>Anstatt einen Manager die Ankündigung machen z...</td>\n",
       "      <td>...</td>\n",
       "      <td>151</td>\n",
       "      <td>95</td>\n",
       "      <td>145</td>\n",
       "      <td>150</td>\n",
       "      <td>85</td>\n",
       "      <td>144</td>\n",
       "      <td>149</td>\n",
       "      <td>76</td>\n",
       "      <td>0.941347</td>\n",
       "      <td>-0.00404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lp                           HITId WorkerId  score    time    system  \\\n",
       "0  de-en  3QQUBC64ZEEFIPT2SKINDZZTZ3SNXN    A0077     78  2052.0  online-G   \n",
       "\n",
       "     type   sid                                          reference  \\\n",
       "0  SYSTEM  1906  Rather than having an executive make the annou...   \n",
       "\n",
       "                                              source  ... chrf:statistics_10  \\\n",
       "0  Anstatt einen Manager die Ankündigung machen z...  ...                151   \n",
       "\n",
       "   chrf:statistics_11  chrf:statistics_12  chrf:statistics_13  \\\n",
       "0                  95                 145                 150   \n",
       "\n",
       "   chrf:statistics_14  chrf:statistics_15  chrf:statistics_16  \\\n",
       "0                  85                 144                 149   \n",
       "\n",
       "   chrf:statistics_17  bert_score:f1  bleurt_score  \n",
       "0                  76       0.941347      -0.00404  \n",
       "\n",
       "[1 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt_scored = pickle.load(open('wmt16-19_toen_scored.pkl', 'rb'))\n",
    "wmt_scored.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SYSTEM    585386\n",
       "REPEAT     75998\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt_scored.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>lp</th>\n",
       "      <th>system</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2016</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">cs-en</th>\n",
       "      <th>PJATK</th>\n",
       "      <td>69.001067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cu-mergedtrees</th>\n",
       "      <td>56.020474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jhu-pbmt</th>\n",
       "      <td>72.411999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online-A</th>\n",
       "      <td>69.633523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online-B</th>\n",
       "      <td>70.983888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">zh-en</th>\n",
       "      <th>online-A.0</th>\n",
       "      <td>75.306259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online-B.0</th>\n",
       "      <td>80.686373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online-G.0</th>\n",
       "      <td>72.351598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online-X.0</th>\n",
       "      <td>66.889482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>online-Y.0</th>\n",
       "      <td>78.002909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>261 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               score\n",
       "year lp    system                   \n",
       "2016 cs-en PJATK           69.001067\n",
       "           cu-mergedtrees  56.020474\n",
       "           jhu-pbmt        72.411999\n",
       "           online-A        69.633523\n",
       "           online-B        70.983888\n",
       "...                              ...\n",
       "2019 zh-en online-A.0      75.306259\n",
       "           online-B.0      80.686373\n",
       "           online-G.0      72.351598\n",
       "           online-X.0      66.889482\n",
       "           online-Y.0      78.002909\n",
       "\n",
       "[261 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_means = wmt_scored.groupby(['year','lp', 'system'])[['score']].mean()\n",
    "true_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1324\n",
      "((2019, 'zh-en'), 'online-X.0', 'MSRA.MASS.6942')\n"
     ]
    }
   ],
   "source": [
    "def pairs(x):\n",
    "    for (year, lp), group in x.groupby(['year', 'lp']):\n",
    "        systems = group.system.unique()\n",
    "        \n",
    "        for i, j in itertools.combinations(systems, 2):\n",
    "            yield (year, lp), i, j\n",
    "            \n",
    "all_pairs = list(pairs(wmt_scored))\n",
    "pairs_2019 = list(pairs(wmt_scored[wmt_scored.year == 2019]))\n",
    "print(len(all_pairs))\n",
    "print(all_pairs[-1])\n",
    "\n",
    "def get_preds(all_pairs, scores, only_2019=False):\n",
    "    if only_2019:\n",
    "        all_pairs = pairs_2019\n",
    "    \n",
    "    preds = np.zeros(len(all_pairs))\n",
    "    if type(scores) == type({}):\n",
    "        for ii, ((year, lp), i, j) in enumerate(all_pairs):\n",
    "            preds[ii] = 1 if scores[year, lp, i] - scores[year, lp, j] > 0 else 0\n",
    "    else:\n",
    "        for ii, ((year, lp), i, j) in enumerate(all_pairs):\n",
    "            preds[ii] = 1 if scores.loc[year, lp, i] - scores.loc[year, lp, j] > 0 else 0\n",
    "            \n",
    "    return preds\n",
    "\n",
    "true_preds = get_preds(all_pairs, true_means['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU\n",
    "def bleu_agg(x):\n",
    "    cols = [ ('bleu:counts_%d' % i) for i in range(0, 4) ] + [ ('bleu:totals_%d' % i) for i in range(0, 4) ]\n",
    "    cols += ['bleu:sys_len', 'bleu:ref_len']\n",
    "    aggs = x[cols].sum()\n",
    "\n",
    "    totals = [ aggs['bleu:totals_%d' % i] for i in range(0, 4) ]\n",
    "    counts = [ aggs['bleu:counts_%d' % i] for i in range(0, 4) ]\n",
    "    ref_len = aggs['bleu:ref_len']\n",
    "    sys_len = aggs['bleu:sys_len']\n",
    "\n",
    "    return sacrebleu.compute_bleu(counts, totals, sys_len, ref_len).score\n",
    "\n",
    "# TER\n",
    "def ter_agg(x):\n",
    "    cols = [ 'ter:num_edits', 'ter:ref_length' ]\n",
    "    aggs = x[cols].sum()\n",
    "\n",
    "    num_edits = aggs['ter:num_edits']\n",
    "    ref_length = aggs['ter:ref_length']\n",
    "\n",
    "    # for one reference\n",
    "    return -num_edits / ref_length\n",
    "\n",
    "\n",
    "# chrf\n",
    "def chrf_agg(x):\n",
    "    cols = [ ('chrf:statistics_%d' % i) for i in range(0, 18) ]\n",
    "    aggs = x[cols].sum()\n",
    "\n",
    "    statistics = [ aggs['chrf:statistics_%d' % i] for i in range(0, 18) ]\n",
    "\n",
    "    return sacrebleu.CHRF.compute_chrf(statistics, order=6, beta=2).score\n",
    "\n",
    "def bert_score_agg(x):\n",
    "    return x['bert_score:f1'].mean()\n",
    "\n",
    "def bleurt_agg(x):\n",
    "    return x['bleurt_score'].mean()\n",
    "\n",
    "def human_agg(x):\n",
    "    return x['score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/bias_variance_decomp.py\n",
    "def bias_var_decomposition(metric_results, true_preds):\n",
    "    main_predictions = np.apply_along_axis(lambda x:\n",
    "                                           np.argmax(np.bincount(x)),\n",
    "                                           axis=0,\n",
    "                                           arr=metric_results)\n",
    "\n",
    "    avg_expected_loss = (true_preds != metric_results).mean()\n",
    "\n",
    "    avg_bias = (main_predictions != true_preds).mean()\n",
    "\n",
    "    signs = (main_predictions == true_preds).astype(np.int64) * 2 - 1\n",
    "    variances = (metric_results != main_predictions).mean(axis=0)\n",
    "    avg_var_contrib = (signs * variances).mean()\n",
    "\n",
    "    return avg_expected_loss, avg_bias, avg_var_contrib\n",
    "\n",
    "# adopted from https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/bias_variance_decomp.py\n",
    "def bias_var_noise_decomposition(metric_results, human_results, true_preds, no_bias=False):\n",
    "    main_predictions = np.apply_along_axis(lambda x:\n",
    "                                           np.argmax(np.bincount(x)),\n",
    "                                           axis=0,\n",
    "                                           arr=metric_results)\n",
    "\n",
    "    optimal_predictions = np.apply_along_axis(lambda x:\n",
    "                                       np.argmax(np.bincount(x)),\n",
    "                                       axis=0,\n",
    "                                       arr=human_results)\n",
    "    \n",
    "    if no_bias:\n",
    "        main_predictions = optimal_predictions\n",
    "    true_preds = optimal_predictions\n",
    "    \n",
    "    avg_expected_loss = (human_results != metric_results).mean()\n",
    "\n",
    "    noises = (human_results != true_preds).mean(axis=0)\n",
    "    probs = (metric_results == true_preds).mean(axis=0)\n",
    "\n",
    "    avg_noise_contrib = ((2 * probs - 1) * noises).mean()\n",
    "\n",
    "    avg_bias = (main_predictions != true_preds).mean()\n",
    "\n",
    "    signs = (main_predictions == true_preds).astype(np.int64) * 2 - 1\n",
    "    variances = (metric_results != main_predictions).mean(axis=0)\n",
    "    avg_var_contrib = (signs * variances).mean()\n",
    "\n",
    "    return avg_expected_loss, avg_bias, avg_var_contrib, avg_noise_contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504d5ebdc5464a2c98afb3e5c9d96169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cached = [ (i, pd.DataFrame(g)) for i, g in wmt_scored.groupby(['year', 'lp', 'system']) ]\n",
    "labels = [ i[0] for i in cached ]\n",
    "\n",
    "def f(metric):\n",
    "    groups = [ g.sample(frac=1, replace=True) for i, g in cached ]\n",
    "    observed_means = [ g['score'].mean() for g in groups ]\n",
    "    \n",
    "    d = { label:mean for label, mean in zip(labels, observed_means) }\n",
    "    return get_preds(all_pairs, d)\n",
    "\n",
    "with Pool(12) as p:\n",
    "    it = tqdm(p.imap_unordered(f, [None]*10000, chunksize=10), total=10000)\n",
    "    output = list(it)\n",
    "    human_results = np.array(output, dtype=np.int64)\n",
    "    \n",
    "# optimal predictions\n",
    "optimal_predictions = np.apply_along_axis(lambda x:\n",
    "                                   np.argmax(np.bincount(x)),\n",
    "                                   axis=0,\n",
    "                                   arr=human_results)\n",
    "\n",
    "optimal_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aee449088c44ba395ecf922324e41d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bc3a016e554ca885d8aeadcd79f8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cached = [ (i, pd.DataFrame(g)) for i, g in wmt_scored.groupby(['year', 'lp', 'system']) ]\n",
    "labels = [ i[0] for i in cached ]\n",
    "\n",
    "def f(metric):    \n",
    "    groups = [ g.sample(frac=1, replace=True) for i, g in cached ]\n",
    "    observed_means = [ g['score'].mean() for g in groups ]\n",
    "    observed_means = { label:mean for label, mean in zip(labels, observed_means) }\n",
    "    \n",
    "    if metric == 'true_preds':\n",
    "        return get_preds(all_pairs, observed_means), optimal_predictions\n",
    "    \n",
    "    # get agg function\n",
    "    agg = globals()['%s_agg' % metric]\n",
    "        \n",
    "    groups = [ g.sample(frac=1, replace=True) for i, g in cached ]\n",
    "    agg_means = [ agg(g) for g in groups ]\n",
    "    agg_means = { label:mean for label, mean in zip(labels, agg_means) }\n",
    "\n",
    "    flag = metric == 'bleurt'\n",
    "    return get_preds(all_pairs, observed_means, only_2019=flag), get_preds(all_pairs, agg_means, only_2019=flag)\n",
    "\n",
    "WORKERS = 12\n",
    "NUM_TRIALS = 10000\n",
    "CHUNKSIZE = int(NUM_TRIALS / WORKERS / 10)\n",
    "metrics = ['bleurt', 'human', 'true_preds', 'bleu', 'ter', 'chrf', 'bert_score']\n",
    "#metrics = ['true_preds', 'human']\n",
    "results = {}\n",
    "\n",
    "with Pool(WORKERS) as p:\n",
    "    for metric in tqdm(metrics):\n",
    "        it = tqdm(p.imap_unordered(f, [metric]*NUM_TRIALS, chunksize=CHUNKSIZE), total=NUM_TRIALS)\n",
    "        output = list(it)\n",
    "\n",
    "        human_results = np.array([ i[0] for i in output ], dtype=np.int64)\n",
    "        metric_results = np.array([ i[1] for i in output ], dtype=np.int64)\n",
    "\n",
    "        flag = metric in ['human', 'true_preds']\n",
    "        results[metric] = bias_var_noise_decomposition(metric_results, human_results, true_preds, no_bias=flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_preds\n",
      "0.04681170694864048\n",
      "0.04681170694864048\n",
      "human\n",
      "0.06536132930513595\n",
      "0.06504519663141993\n",
      "bert_score\n",
      "0.10233723564954683\n",
      "0.1024389599244713\n",
      "chrf\n",
      "0.12428489425981873\n",
      "0.12421669290030211\n",
      "bleurt\n",
      "0.1280742388758782\n",
      "0.12800524426229506\n",
      "bleu\n",
      "0.14136503021148036\n",
      "0.14128590099697885\n",
      "ter\n",
      "0.18422069486404835\n",
      "0.18412075448640483\n"
     ]
    }
   ],
   "source": [
    "for metric, (avg_expected_loss, avg_bias, avg_var_contrib, avg_noise_contrib) in sorted(list(results.items()), key=lambda x: x[1][0]):\n",
    "    print(metric)\n",
    "    print(avg_expected_loss)\n",
    "    print(avg_noise_contrib + avg_bias + avg_var_contrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_preds & 0.047 & 0.045 & {\\bf 0.000 } & 0.002 \\\\\n",
      "human & 0.065 & 0.019 & 0.000 & {\\bf 0.047 } \\\\\n",
      "bert_score & 0.102 & 0.012 & {\\bf 0.086 } & 0.004 \\\\\n",
      "chrf & 0.124 & 0.008 & {\\bf 0.106 } & 0.010 \\\\\n",
      "bleurt & 0.128 & 0.012 & {\\bf 0.112 } & 0.004 \\\\\n",
      "bleu & 0.141 & 0.006 & {\\bf 0.127 } & 0.008 \\\\\n",
      "ter & 0.184 & 0.009 & {\\bf 0.173 } & 0.002 \\\\\n"
     ]
    }
   ],
   "source": [
    "for metric, (avg_expected_loss, avg_bias, avg_var_contrib, avg_noise_contrib) in sorted(list(results.items()), key=lambda x: x[1][0]):\n",
    "    if metric == 'human':\n",
    "        s = '%s & %.3f & %.3f & %.3f & {\\\\bf %.3f } \\\\\\\\'\n",
    "    elif metric =='true_pred':\n",
    "        s = '%s & %.3f & {\\\\bf %.3f } & %.3f & %.3f \\\\\\\\'\n",
    "    else:\n",
    "        s = '%s & %.3f & %.3f & {\\\\bf %.3f } & %.3f \\\\\\\\'\n",
    "    print(s % (metric, avg_expected_loss, (avg_expected_loss-avg_bias-avg_var_contrib), avg_bias, avg_var_contrib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(results, open('bvd_results_wmt.json', 'wt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
